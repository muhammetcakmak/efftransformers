# Comparing with Grapes datasets
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.datasets import ImageFolder
from efficientnet_pytorch import EfficientNet
from transformers import ViTForImageClassification
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
from tqdm import tqdm

# Creating feature extractor with EfficientNet
class EfficientNetFeatureExtractor(nn.Module):
    def __init__(self, model_name='efficientnet-b0'):
        super(EfficientNetFeatureExtractor, self).__init__()
        self.efficientnet = EfficientNet.from_pretrained(model_name)
        self.pool = nn.AdaptiveAvgPool2d(1)
        
        # Get the number of features from the feature extractor
        dummy_input = torch.randn(1, 3, 224, 224)
        self.num_features = self._get_num_features(dummy_input)

    def _get_num_features(self, x):
        x = self.efficientnet.extract_features(x)
        x = self.pool(x)
        return x.view(x.size(0), -1).shape[1]

    def forward(self, x):
        x = self.efficientnet.extract_features(x)
        x = self.pool(x)
        return x.view(x.size(0), -1)

# Hybrid Model
class HybridModel(nn.Module):
    def __init__(self, num_classes):
        super(HybridModel, self).__init__()
        self.efficientnet_feature_extractor = EfficientNetFeatureExtractor()
        self.vit_feature_extractor = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224-in21k")
        
        # Get the output size of ViT
        dummy_input = torch.randn(1, 3, 224, 224)
        vit_output = self.vit_feature_extractor(dummy_input)
        vit_feature_size = vit_output.logits.shape[-1]

        # Fully connected layers
        self.fc1 = nn.Linear(self.efficientnet_feature_extractor.num_features + vit_feature_size, 512)
        self.fc2 = nn.Linear(512, num_classes)
    
    def forward(self, x):
        efficientnet_features = self.efficientnet_feature_extractor(x)
        vit_output = self.vit_feature_extractor(x)
        vit_features = vit_output.logits.view(vit_output.logits.size(0), -1)
        combined_features = torch.cat((efficientnet_features, vit_features), dim=1)
        
        x = torch.relu(self.fc1(combined_features))
        x = self.fc2(x)
        return x

# Data Loader function
def load_images(data_path, target_size=(224, 224)):
    transform = transforms.Compose([
        transforms.Resize(target_size),
        transforms.ToTensor(),
    ])
    dataset = ImageFolder(data_path, transform=transform)
    data_loader = DataLoader(dataset, batch_size=32, shuffle=True)
    return data_loader

# Data Paths
train_data_path = "datasets/Grape_Dataset/train/"
test_data_path = "datasets/Grape_Dataset/validation/"

# Data Loaders
train_loader = load_images(train_data_path)
test_loader = load_images(test_data_path)

# Model, Loss function, and Optimization
num_classes = len(train_loader.dataset.classes)
hybrid_model = HybridModel(num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(hybrid_model.parameters(), lr=2e-5)

# Lists to track training and test accuracies
train_accuracies = []
test_accuracies = []

# Device handling
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
hybrid_model.to(device)

# Training loop
num_epochs = 10
for epoch in range(num_epochs):
    hybrid_model.train()
    total_loss = 0.0
    for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = hybrid_model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    average_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{num_epochs}, Average Loss: {average_loss}")

    # Calculate test accuracy
    hybrid_model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc="Testing"):
            images, labels = images.to(device), labels.to(device)
            outputs = hybrid_model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    test_accuracy = correct / total
    test_accuracies.append(test_accuracy)
    print(f'Test Accuracy: {test_accuracy * 100:.2f}%')

# Plot test accuracy
plt.plot(range(1, num_epochs+1), test_accuracies, label='Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Test Accuracy Over Epochs')
plt.legend()
plt.show()

# Generate classification report
hybrid_model.eval()
all_labels = []
all_predictions = []
with torch.no_grad():
    for images, labels in tqdm(test_loader, desc="Generating Classification Report"):
        images, labels = images.to(device), labels.to(device)
        outputs = hybrid_model(images)
        _, predicted = torch.max(outputs, 1)
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

print("Classification Report:")
print(classification_report(all_labels, all_predictions, target_names=train_loader.dataset.classes))
